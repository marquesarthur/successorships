\subsection{Replication Strategy and Consistency Guarantees}
\label{sub:approach_replication_strategy}

\textbf{State replication.}
All \APIshort nodes share a versioned global state during the lifetime of the app.
The app is ``alive'' as long as there is one node that advertises the app's service in the local network.
Each time the state is changed on the server node, the state's version is incremented.
The current state is obtained initially by clients triggered by the first handshake with the current server: whenever a new client connects, a WebSocket \texttt{open} event is triggered on the server and a new unique client ID is added to the \texttt{successors} list in the global state object.
The changed state is then broadcasted to all clients\footnote{Note that we are broadcasting the full state in this case rather than a single operation.
A possibly better solution could be that we broadcast only a \texttt{successoradded} operation to all clients except the new one and include the full state for the new client in the \texttt{welcome} message along with the newly created client ID. We come back to this problem in section~\ref{sec:limitations_and_future_work}.}, including the new one.
This ensures that any new client is up-to-date shortly after connecting and that all other clients will have the new client in their \texttt{successors} list.
After a client is connected, it can invoke state changes on the server by calling \texttt{Shippy.call} which will result in a WebSocket message to the server where the submitted operation is applied.
After an operation was applied on the server, this operation is broadcasted to all clients in the order as they appear in the \texttt{successors} list in the shared state.

Since the state is always changed on the current server first and changes are then broadcasted to clients, clients can simply apply the changes locally in ordinary cases.
However, there can be exceptions in certain failure scenarios due to race conditions and the asynchronous nature of WebSocket broadcasting.
We cope with such failures by versioning the state: when clients receive state changes (by listening to \texttt{stateupdate} events), they will check the version of the server's current state and compare it with the version of their local state.
In case their local state is newer than the server state, they will send a \texttt{\_aheadofserver}\footnote{We precede messages not associated with app-defined operations with an underscore.} message to the server with their current local state as payload.
The server can then accept this newer state and again broadcast a state update.
Note that even if several clients send \texttt{\_aheadofserver} messages, the server will only accept these if their state's version is higher than its own, which indicates that consensus will be reached eventually.
In the current \APIshort implementation, we consider clients being ``ahead'' of the server a hypothetical scenario that we could not produce in any of our evaluated use-cases.


\textbf{Reaching consensus.}
We leverage the concepts of \textit{eventual consistency} and \textit{lazy replication} in Shippy \cite{Ladin:1990,Ladin:1992}.
Briefly, this means that nodes holding replicas of the state periodically exchange information, tolerating out-of-sync periods; thus, state may diverge across nodes, but is guaranteed to converge when the system quiesces for some period of time.

We argue that our domain of local ad-hoc offline Web applications will mostly tolerate minor time frames of inconsistent states and we therefore focus on availability and performance rather than accepting the overhead induced by stronger consistency principles.
With the replication strategy previously described, the state manipulation can only happen on the current server and clients are only notified of changes applied on the server.
The most obvious source for inconsistencies would be lost messages carrying operations from server to client and vice versa.
However, this potential problem is mostly mitigated by the design of the WebSocket protocol: each client is connected to the server with a stable bi-directional channel. 
Whenever this channel is interrupted, an \texttt{error} or \texttt{close} event will be triggered on both the client and the server side of the channel.
In such cases, the server will simply remove the client both from its maintained collection of WebSocket clients and the \texttt{successors} list on the shared state (resulting in a \texttt{stateupdate} broadcast). 
The client on the other hand, will try to create a new connection to the server, which will then result in obtaining a fresh copy of the server state.
While we are certain that all nodes will eventually converge to a consistent state with our approach in most scenarios, we cannot eliminate the risk of inconsistencies in certain edge cases (see section~\ref{sec:limitations_and_future_work}).